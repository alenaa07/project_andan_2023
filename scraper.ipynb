{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests, sys, time, os, argparse\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vbD9m5lPoYCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of simple to collect features\n",
        "snippet_features = [\"title\",\n",
        "                    \"publishedAt\",\n",
        "                    \"channelId\",\n",
        "                    \"channelTitle\",\n",
        "                    \"categoryId\"]\n",
        "\n",
        "# Any characters to exclude, generally these are things that become problematic in CSV files\n",
        "unsafe_characters = ['\\n', '\"']\n",
        "\n",
        "# Used to identify columns, currently hardcoded order\n",
        "header = [\"video_id\"] + snippet_features + [\"trending_date\", \"tags\", \"view_count\", \"likes\", \"dislikes\",\n",
        "                                            \"comment_count\", \"thumbnail_link\", \"comments_disabled\",\n",
        "                                            \"ratings_disabled\", \"description\"]\n",
        "\n",
        "\n",
        "def setup(api_path, code_path):\n",
        "    with open(api_path, 'r') as file:\n",
        "        api_key = file.readline()\n",
        "\n",
        "    with open(code_path) as file:\n",
        "        country_codes = [x.rstrip() for x in file]\n",
        "\n",
        "    return api_key, country_codes\n",
        "\n",
        "\n",
        "def prepare_feature(feature):\n",
        "    # Removes any character from the unsafe characters list and surrounds the whole item in quotes\n",
        "    for ch in unsafe_characters:\n",
        "        feature = str(feature).replace(ch, \"\")\n",
        "    return f'\"{feature}\"'\n",
        "\n",
        "\n",
        "def api_request(page_token, country_code):\n",
        "    # Builds the URL and requests the JSON from it\n",
        "    request_url = f\"https://www.googleapis.com/youtube/v3/videos?part=id,statistics,snippet{page_token}chart=mostPopular&regionCode={country_code}&maxResults=50&key={api_key}\"\n",
        "    request = requests.get(request_url)\n",
        "    if request.status_code == 429:\n",
        "        print(\"Temp-Banned due to excess requests, please wait and continue later\")\n",
        "        sys.exit()\n",
        "    return request.json()\n",
        "\n",
        "\n",
        "def get_tags(tags_list):\n",
        "    # Takes a list of tags, prepares each tag and joins them into a string by the pipe character\n",
        "    return prepare_feature(\"|\".join(tags_list))\n",
        "\n",
        "\n",
        "def get_videos(items):\n",
        "    lines = []\n",
        "    for video in items:\n",
        "        comments_disabled = False\n",
        "        ratings_disabled = False\n",
        "\n",
        "        # We can assume something is wrong with the video if it has no statistics, often this means it has been deleted\n",
        "        # so we can just skip it\n",
        "        if \"statistics\" not in video:\n",
        "            continue\n",
        "\n",
        "        # A full explanation of all of these features can be found on the GitHub page for this project\n",
        "        video_id = prepare_feature(video['id'])\n",
        "\n",
        "        # Snippet and statistics are sub-dicts of video, containing the most useful info\n",
        "        snippet = video['snippet']\n",
        "        statistics = video['statistics']\n",
        "\n",
        "        # This list contains all of the features in snippet that are 1 deep and require no special processing\n",
        "        features = [prepare_feature(snippet.get(feature, \"\")) for feature in snippet_features]\n",
        "\n",
        "        # The following are special case features which require unique processing, or are not within the snippet dict\n",
        "        description = snippet.get(\"description\", \"\")\n",
        "        thumbnail_link = snippet.get(\"thumbnails\", dict()).get(\"default\", dict()).get(\"url\", \"\")\n",
        "        trending_date = time.strftime(\"%y.%d.%m\")\n",
        "        tags = get_tags(snippet.get(\"tags\", [\"[none]\"]))\n",
        "        view_count = statistics.get(\"viewCount\", 0)\n",
        "\n",
        "        # This may be unclear, essentially the way the API works is that if a video has comments or ratings disabled\n",
        "        # then it has no feature for it, thus if they don't exist in the statistics dict we know they are disabled\n",
        "        if 'likeCount' in statistics:\n",
        "            likes = statistics['likeCount']\n",
        "            dislikes = 0\n",
        "        else:\n",
        "            ratings_disabled = True\n",
        "            likes = 0\n",
        "            dislikes = 0\n",
        "\n",
        "        if 'commentCount' in statistics:\n",
        "            comment_count = statistics['commentCount']\n",
        "        else:\n",
        "            comments_disabled = True\n",
        "            comment_count = 0\n",
        "\n",
        "        # Compiles all of the various bits of info into one consistently formatted line\n",
        "        line = [video_id] + features + [prepare_feature(x) for x in [trending_date, tags, view_count, likes, dislikes,\n",
        "                                                                       comment_count, thumbnail_link, comments_disabled,\n",
        "                                                                       ratings_disabled, description]]\n",
        "        lines.append(\",\".join(line))\n",
        "    return lines\n",
        "\n",
        "\n",
        "def get_pages(country_code, next_page_token=\"&\"):\n",
        "    country_data = []\n",
        "\n",
        "    # Because the API uses page tokens (which are literally just the same function of numbers everywhere) it is much\n",
        "    # more inconvenient to iterate over pages, but that is what is done here.\n",
        "    while next_page_token is not None:\n",
        "        # A page of data i.e. a list of videos and all needed data\n",
        "        video_data_page = api_request(next_page_token, country_code)\n",
        "\n",
        "        # Get the next page token and build a string which can be injected into the request with it, unless it's None,\n",
        "        # then let the whole thing be None so that the loop ends after this cycle\n",
        "        next_page_token = video_data_page.get(\"nextPageToken\", None)\n",
        "        next_page_token = f\"&pageToken={next_page_token}&\" if next_page_token is not None else next_page_token\n",
        "\n",
        "        # Get all of the items as a list and let get_videos return the needed features\n",
        "        items = video_data_page.get('items', [])\n",
        "        country_data += get_videos(items)\n",
        "\n",
        "    return country_data\n",
        "\n",
        "\n",
        "def write_to_file(country_code, country_data):\n",
        "\n",
        "    print(f\"Writing {country_code} data to file...\")\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    with open(f\"{output_dir}/{time.strftime('%y.%d.%m')}_{country_code}_videos.csv\", \"w+\", encoding='utf-8') as file:\n",
        "        for row in country_data:\n",
        "            file.write(f\"{row}\\n\")\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    for country_code in country_codes:\n",
        "        country_data = [\",\".join(header)] + get_pages(country_code)\n",
        "        write_to_file(country_code, country_data)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--key_path', help='Path to the file containing the api key, by default will use api_key.txt in the same directory', default='api_key.txt')\n",
        "    parser.add_argument('--country_code_path', help='Path to the file containing the list of country codes to scrape, by default will use country_codes.txt in the same directory', default='country_codes.txt')\n",
        "    parser.add_argument('--output_dir', help='Path to save the outputted files in', default='output/')\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    output_dir = args.output_dir\n",
        "    api_key, country_codes = setup(args.key_path, args.country_code_path)\n",
        "\n",
        "    get_data()"
      ],
      "metadata": {
        "id": "_w48BlN-lNFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "country_decode = { 'US': 'United States', 'DE': 'Germany', 'CA': 'Canada', 'FR': 'France', 'KR': 'South Korea', 'PT': 'Portugal', 'ES': 'Spain', 'BE': 'Belgium',\n",
        "    'GB': 'United Kingdom', 'IE': 'Ireland', 'IS': 'Iceland', 'IT': 'Italy', 'AT': 'Austria', 'CZ': 'Czech Republic', 'PL': 'Poland', 'UA': 'Ukraine',\n",
        "    'NO': 'Norway', 'SE': 'Sweden', 'FI': 'Finland', 'EE': 'Estonia', 'LV': 'Latvia', 'LT': 'Lithuania', 'BY': 'Belarus', 'GR': 'Greece', 'DK': 'Denmark',\n",
        "    'GE': 'Georgia', 'AU': 'Australia', 'BG': 'Bulgaria', 'HR': 'Croatia', 'HU': 'Hungary', 'LI': 'Liechtenstein', 'MD': 'Moldova', 'NZ': 'New Zealand',\n",
        "    'RO': 'Romania', 'RS': 'Serbia'}"
      ],
      "metadata": {
        "id": "YcGGM6NJ9-ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('country_codes.txt', 'r', encoding = 'utf-8') as file:\n",
        "  baza = list(map(lambda x: x.strip(), file.readlines()))\n",
        "res = []\n",
        "df = pd.read_csv(f'/content/output/23.14.05_{baza[0]}_videos.csv', sep=',') # в графе расположения файла следует прописывать дату сбора информации\n",
        "df['country'] = np.full(df.shape[0], country_decode[baza[0]]) # добавление в датасет нового столбца - название страны\n",
        "for name in baza[1:]:\n",
        "  path = '/content/output/23.14.05_' + name + '_videos.csv'\n",
        "  df_temp = pd.read_csv(path, sep=',')\n",
        "  df_temp['country'] = np.full(df_temp.shape[0], country_decode[name]) \n",
        "  df = pd.concat([df, df_temp])"
      ],
      "metadata": {
        "id": "HPnBNeO-n2uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "\n",
        "with open('/content/api_key.txt', 'r', encoding = 'utf-8') as file:\n",
        "  api_val = file.readline().strip()\n",
        "\n",
        "api_key = api_val\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "def get_sub_value(chan_id): # функция, которая получает количество подписчиков ютуб канала\n",
        "  response = youtube.channels().list(part='statistics', id = chan_id).execute()\n",
        "\n",
        "  subscriber_count = response['items'][0]['statistics']['subscriberCount']\n",
        "  return subscriber_count"
      ],
      "metadata": {
        "id": "iunxAKQ-znm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['subscriber_count'] = df['channelId'].apply(lambda x: get_sub_value(x)) # добавление нового признака - количества подписчиков канала"
      ],
      "metadata": {
        "id": "_F1uPtPx2M59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('dataset.csv', index= False)"
      ],
      "metadata": {
        "id": "LRpre-plD5Lh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}